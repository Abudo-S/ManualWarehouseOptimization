{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a21c485",
   "metadata": {},
   "source": [
    "### Model Overview\n",
    "\n",
    "* Input: A Heterogeneous Graph object (data) containing:\n",
    "\n",
    "    - data['order'].x: Features for orders.\n",
    "\n",
    "    - data['operator'].x: Features for operators.\n",
    "\n",
    "    - data['order', 'to', 'order'].edge_attr: Distance/Time between orders.\n",
    "\n",
    "    - data['operator', 'to', 'order'].edge_attr: Distance/Time from ops to orders.\n",
    "\n",
    "* Encoder: Distinct MLPs to embed initial features of Orders and Operators into a common hidden dimension.\n",
    "\n",
    "* Processor (Message Passing): Multi-layer Heterogeneous Graph Attention (HeteroGAT) or HeteroGraphConv. This allows Operators to \"see\" Orders and Orders to \"see\" other Orders.\n",
    "\n",
    "* Decoders (Dual Heads):\n",
    "\n",
    "    - Activation Head: Predicts probability $P(activate)$ for each operator node.\n",
    "\n",
    "    - Assignment/Scheduling Head: Predicts a score $P(assign)$ for every (Operator, Order) edge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a98d7c",
   "metadata": {},
   "source": [
    "### Key Architectural Decisions\n",
    "\n",
    "* Heterogeneous Graph (HeteroConv):  \n",
    "    Our problem has two distinct entities: Orders and Operators. Using a homogeneous graph (treating them all as generic \"nodes\") would force the network to \"re-learn\" which nodes are which.\n",
    "\n",
    "    HeteroConv allows us to define specific message-passing rules. For example, ('operator', 'to', 'order') messages explicitly represent \"Resource Feasibility,\" while ('order', 'to', 'order') messages represent \"Sequence/Travel Logic.\"\n",
    "\n",
    "* Edge Features (edge_attr):  \n",
    "Crucially, the GATv2Conv layers are initialized with edge_dim=hidden_dim. This allows the network to use the Travel Time ($T_{jk}$ or $T_{ij}$) as a core part of the attention mechanism. The network learns to \"pay less attention\" (lower weight) to orders that are physically far away, mimicking the logic of minimizing travel time ($\\alpha$).\n",
    "\n",
    "* Dual Heads (TO BE EXPLAINED FURTHER):\n",
    "\n",
    "    - Activation Head: Directly maps the final operator embedding to a probability. Since the embedding has aggregated information from nearby Orders (via the order -> operator layers), the operator \"knows\" if there is high demand nearby, allowing it to accurately predict its own activation status ($y_i$).\n",
    "\n",
    "    - Assignment Head: Instead of just dot-producting node embeddings, we concatenate [Op_Emb, Order_Emb, Travel_Time]. This ensures the final decision explicitly accounts for the static distance ($T_{ij}$), which is vital for the Scheduling Loss ($L_{Scheduling}$).\n",
    "\n",
    "* Scaling: This architecture works on batches of graphs, regadless the graph size. (ex. For the \"10,000 orders\" scaling plan (Cluster -> Solve), we simply feed this model sub-graphs (clusters) of 50-100 orders. The architecture remains exactly the same).\n",
    "\n",
    "##### How to Train\n",
    "1. Data: Load a batch of (Graph, Labels) from our MIP dataset.\n",
    "2. Forward: pred_act, pred_assign = model(graph.x_dict, ...)\n",
    "3. Loss: loss = weighted_loss(pred_act, batch.y_activation, pred_assign, batch.y_assignment, alpha, beta)\n",
    "4. Backward: loss.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb81baf1",
   "metadata": {},
   "source": [
    "### Conditioned Message passing \n",
    "To handle $H_{fixed}$ (Capacity), $\\alpha$ (Makespan weight), and $\\beta$ (Activation weight) explicitly, we need to force the network to mathematically respect these parameters. The best approach is to use Conditioned Message Passing (also called Modulated Attention) and Capacity Masking.\n",
    "\n",
    "#### The architectural strategy to handle these three variables explicitly:\n",
    "\n",
    "1. Handling Global Objectives ($\\alpha, \\beta$): The \"Objective Lens\"\n",
    "\n",
    "    Instead of just feeding $\\alpha$ and $\\beta$ as inputs, use them to modulate the edge weights before the message passing starts. Think of this as putting on \"tinted glasses\" that change how the network sees distance.  \n",
    "\n",
    "    #### Mechanism: Dynamic Edge Scaling\n",
    "\n",
    "    In a standard GNN, the edge feature is just $T_{ij}$ (Travel Time). In our Explicit architecture, we transform the edge features based on $\\alpha$.\n",
    "\n",
    "    * Logic: If $\\alpha$ (time penalty) is high, long edges should effectively act as \"walls\" (very low attention scores).\n",
    "\n",
    "    * Implementation: Inside the Edge Model ($\\phi^e$) of the Meta-Layer:\n",
    "\n",
    "    $$e_{ij}' = \\text{MLP}_{\\text{edge}}(e_{ij} \\cdot (1 + w_1 \\cdot \\alpha))$$\n",
    "    Or, more explicitly in the Attention Mechanism:\n",
    "    $$\\text{Attention}_{ij} = \\text{Softmax} \\left( \\frac{h_i^T W h_j - (\\gamma \\cdot \\alpha \\cdot T_{ij})}{\\sqrt{d}} \\right)$$\n",
    "\n",
    "    Where $T_{ij}$ is the raw travel time. And The term $- (\\gamma \\cdot \\alpha \\cdot T_{ij})$ explicitly subtracts from the attention score proportional to the makespan weight.  \n",
    "    So as a Result: When $\\alpha$ is high, the GNN physically cannot attend to distant nodes effectively, forcing it to pick closer neighbors.  \n",
    "\n",
    "2. Handling Activation Cost ($\\beta$): The \"Activation Gate\"\n",
    "\n",
    "    $\\beta$ dictates the penalty for \"waking up\" a new operator. We should use this to gate the Activation Head of the network.\n",
    "    Mechanism: Bias Injection\n",
    "\n",
    "    The final output layer for the Activation Policy ($P_{activate}$) usually looks like Sigmoid(Wx + b). We can explicitly inject $\\beta$ into the bias term.\n",
    "\n",
    "    $$P_{\\text{activate}}(i) = \\sigma \\left( \\text{MLP}(h_i) - (\\lambda \\cdot \\beta \\cdot I_{i, \\text{s\\_new}}) \\right)$$\n",
    "\n",
    "    Where $I_{i, \\text{s\\_new}}$ is 1 if the operator is currently idle, 0 if already active.\n",
    "\n",
    "    So as a result: As $\\beta$ increases, the output probability is mathematically pushed downward (subtracted). The operator's embedding $\\mathbf{h}_i$ must be extremely strong (i.e., \"I am perfectly positioned for these orders\") to overcome the negative $\\beta$ bias.  \n",
    "\n",
    "3. Handling Capacity ($H_{fixed}$): The \"Fuel Gauge\"\n",
    "\n",
    "    $H_{fixed}$ is a hard constraint for the Operator. In a constructive (step-by-step) GNN, simply knowing the static $H_{fixed}$ isn't enough; the model needs to know the Utilization Ratio.  \n",
    "\n",
    "    #### Mechanism A: Dynamic Feature Engineering (Input Level)\n",
    "\n",
    "    At every step of the GNN (during the constructive rollout), update the Operator Node features to include:\n",
    "\n",
    "    $$h_{op} = \\left[ \\dots, H_{\\text{fixed}}, H_{\\text{current\\_load}}, \\frac{H_{\\text{current\\_load}}}{H_{\\text{fixed}}} \\right]$$\n",
    "\n",
    "    The Ratio ($\\frac{H_{load}}{H_{fixed}}$): This is the most critical feature. It normalizes the capacity. The GNN learns generic rules like \"If Ratio > 0.9, stop assigning.\"\n",
    "\n",
    "    #### Mechanism B: Hard Constraint Masking (Output Level)\n",
    "\n",
    "    While the GNN learns soft rules, we must enforce hard rules during inference (and training, if using simple Greedy rollout).\n",
    "\n",
    "    When calculating the Assignment Scores (Output Layer), apply a Validity Mask:\n",
    "\n",
    "    $$\\text{Score}(i,j) = \n",
    "    \\begin{cases} \n",
    "    \\text{GNN\\_Output}(i,j) & \\text{if } (H_{\\text{current\\_load}}(i) + \\text{Size}(j)) \\leq H_{\\text{fixed}}(i) \\\\\n",
    "    -\\infty & \\text{otherwise}\n",
    "    \\end{cases}$$\n",
    "\n",
    "    Result: It becomes physically impossible for the GNN to assign an order that violates $H_{fixed}$, regardless of what the neural weights say"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
