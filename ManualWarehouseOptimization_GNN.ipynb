{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e43a612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abudo\\source\\vscode_projects\\ManualWarehouseOptimization\\cplex_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, BisectingKMeans\n",
    "import importlib\n",
    "import math\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from k_means_constrained import KMeansConstrained\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "import GnnScheduleDataset as GnnScheduleDataset_Module\n",
    "import MultiCriteriaGNNModel as MultiCriteriaGNNModel_Module\n",
    "\n",
    "importlib.reload(GnnScheduleDataset_Module) # in case of updates\n",
    "importlib.reload(MultiCriteriaGNNModel_Module) # in case of updates\n",
    "\n",
    "from GnnScheduleDataset import GnnScheduleDataset\n",
    "from MultiCriteriaGNNModel import MultiCriteriaGNNModel\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "#manual installing\n",
    "def install_package(package_name, use_index_url=True):\n",
    "    print(f\"Installing {package_name}...\")\n",
    "    #run: python.exe -m pip install [package_name]\n",
    "\n",
    "    subprocess.run(\n",
    "        [sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", package_name],\n",
    "        check=True,\n",
    "        text=True\n",
    "    )\n",
    "    if use_index_url:\n",
    "        subprocess.run(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", package_name, \"--index-url\", \"https://download.pytorch.org/whl/cu126\"],\n",
    "            check=True,\n",
    "            text=True\n",
    "        )\n",
    "    else:\n",
    "        subprocess.run(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", package_name],\n",
    "            check=True,\n",
    "            text=True\n",
    "        )\n",
    "    print(f\"Successfully installed {package_name}!\")\n",
    "\n",
    "# Try to import, if it fails, install it\n",
    "# try:\n",
    "#     import torch\n",
    "#     print(\"Torch is already available.\")\n",
    "# except: \n",
    "#     #install_package('torch')\n",
    "#     # After installing, you must use importlib to refresh or restart the script\n",
    "#     import torch\n",
    "#     print(\"Torch imported successfully after installation.\")\n",
    "\n",
    "#torch-scatter torch-sparse torch-cluster torch-spline-conv pyg-lib\n",
    "#torch-scatter torch-sparse torch-cluster torch-spline-conv\n",
    "# install_package('torch-scatter')\n",
    "# install_package('torch-sparse')\n",
    "# install_package('torch-cluster')\n",
    "# install_package('torch-spline-conv')\n",
    "\n",
    "# try:\n",
    "#     import torch_geometric\n",
    "#     print(\"Torch is already available.\")\n",
    "# except: \n",
    "#     install_package('torch_geometric', False)\n",
    "#     # After installing, you must use importlib to refresh or restart the script\n",
    "#     import torch_geometric\n",
    "#     print(\"Torch imported successfully after installation.\")\n",
    "\n",
    "#Try to import, if it fails, install it\n",
    "# try:\n",
    "#     import k_means_constrained\n",
    "#     print(\"k_means_constrained is already available.\")\n",
    "# except: \n",
    "#     install_package('k_means_constrained', False)\n",
    "#     # After installing, you must use importlib to refresh or restart the script\n",
    "#     import torch\n",
    "#     print(\"k_means_constrained imported successfully after installation.\")\n",
    "\n",
    "#file paths\n",
    "TARGET_BATCH_SIZE = 10 #number of missions per mini-batch\n",
    "LARGE_SCALE_MISSION_BATCH_DIR = \"./datasets/Batch1000M.csv\"\n",
    "PREPROCESSED_BATCH_DIR = f\"./preprocessed/batch1000M/Batch{TARGET_BATCH_SIZE}M_idx.xlsx\" #idx to be replaced cluster idx\n",
    "MISSION_BATCH_DIR = \"./datasets/mini-batch/Batch10M_distanced.csv\"\n",
    "UDC_TYPES_DIR = \"./datasets/WM_UDC_TYPE.csv\"\n",
    "MISSION_BATCH_TRAVEL_DIR = \"./datasets/mini-batch/Batch10M_travel_distanced.csv\"\n",
    "FORK_LIFTS_DIR = \"./datasets/ForkLifts10W.csv\"\n",
    "#MISSION_TYPES_DIR = \"./datasets/MissionTypes.csv\"\n",
    "SCHEDULE_DIR = \"./schedules/mini-batch/\"\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 92 #nice to be equal to the number of mini-batch instances\n",
    "LEARNING_RATE = 0.001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0325e6",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Split the large-scale batch in mini-batches (ex. 10 missions each) using spatial k-means, considering only `FROM_X, FROM_Y, FROM_Z, TO_X, TO_Y, TO_Z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05824876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total orders: 920 | target clusters: 92\n",
      "saved ./preprocessed/batch1000M/Batch10M_1.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_2.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_3.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_4.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_5.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_6.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_7.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_8.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_9.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_10.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_11.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_12.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_13.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_14.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_15.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_16.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_17.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_18.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_19.xlsx with 10 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_20.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_21.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_22.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_23.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_24.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_25.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_26.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_27.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_28.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_29.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_30.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_31.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_32.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_33.xlsx with 10 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_34.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_35.xlsx with 10 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_36.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_37.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_38.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_39.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_40.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_41.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_42.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_43.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_44.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_45.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_46.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_47.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_48.xlsx with 10 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_49.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_50.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_51.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_52.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_53.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_54.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_55.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_56.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_57.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_58.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_59.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_60.xlsx with 10 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_61.xlsx with 10 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_62.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_63.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_64.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_65.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_66.xlsx with 10 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_67.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_68.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_69.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_70.xlsx with 10 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_71.xlsx with 10 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_72.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_73.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_74.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_75.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_76.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_77.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_78.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_79.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_80.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_81.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_82.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_83.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_84.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_85.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_86.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_87.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_88.xlsx with 10 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_89.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_90.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_91.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_92.xlsx with 9 missions.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./preprocessed/batch1000M/Batch10M_1.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_2.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_3.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_4.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_5.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_6.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_7.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_8.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_9.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_10.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_11.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_12.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_13.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_14.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_15.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_16.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_17.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_18.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_19.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_20.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_21.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_22.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_23.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_24.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_25.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_26.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_27.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_28.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_29.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_30.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_31.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_32.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_33.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_34.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_35.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_36.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_37.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_38.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_39.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_40.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_41.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_42.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_43.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_44.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_45.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_46.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_47.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_48.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_49.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_50.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_51.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_52.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_53.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_54.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_55.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_56.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_57.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_58.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_59.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_60.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_61.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_62.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_63.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_64.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_65.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_66.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_67.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_68.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_69.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_70.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_71.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_72.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_73.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_74.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_75.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_76.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_77.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_78.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_79.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_80.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_81.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_82.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_83.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_84.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_85.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_86.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_87.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_88.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_89.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_90.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_91.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_92.xlsx']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_spatial_batches_and_save(input_csv=LARGE_SCALE_MISSION_BATCH_DIR, target_batch_size=TARGET_BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    groups missions into spatially dense clusters of <50 missions \n",
    "    and saves each group to a separate CSV file.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(PREPROCESSED_BATCH_DIR), exist_ok=True)\n",
    "    \n",
    "    #load the main dataset\n",
    "    df = pd.read_csv(input_csv).drop_duplicates(subset=['CD_MISSION'], keep='first')\n",
    "    df = df.reset_index(drop=True)\n",
    "    n_samples = len(df)\n",
    "    \n",
    "    #determine number of clusters\n",
    "    n_clusters = max(1, math.ceil(n_samples / target_batch_size))\n",
    "    print(f\"total orders: {n_samples} | target clusters: {n_clusters}\")\n",
    "\n",
    "    if n_clusters == 1:\n",
    "        df['cluster_id'] = 0\n",
    "        df.to_csv('cluster_0.csv', index=False)\n",
    "        return [\"cluster_0.csv\"]\n",
    "\n",
    "    #feature Selection: 6D spatial coordinates\n",
    "    #use both pickup (from) and drop-off (to) locations\n",
    "    coord_cols = ['FROM_X', 'FROM_Y', 'FROM_Z', 'TO_X', 'TO_Y', 'TO_Z']\n",
    "    \n",
    "    for col in coord_cols:\n",
    "        if df[col].dtype == 'object' or df[col].dtype == 'string':  #only apply to string/object columns\n",
    "            df[col] = df[col].str.replace(',', '', regex=False)\n",
    "\n",
    "    coords = df[coord_cols].astype(float).fillna(0).values\n",
    "    \n",
    "    #standardize (crucial for K-Means distance calculations)\n",
    "    scaler = StandardScaler()\n",
    "    coords_scaled = scaler.fit_transform(coords)\n",
    "    \n",
    "    #cluster logic\n",
    "    if n_clusters > 1:\n",
    "        #kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10) #produces some very unbalanced clusters\n",
    "        #df['cluster_id'] = kmeans.fit_predict(coords_scaled)\n",
    "\n",
    "        clf = KMeansConstrained(\n",
    "            n_clusters=n_clusters,\n",
    "            size_min=target_batch_size - 1,\n",
    "            size_max=target_batch_size + 1,\n",
    "            random_state=42\n",
    "        )\n",
    "        df['cluster_id'] = clf.fit_predict(coords_scaled)\n",
    "\n",
    "        # model = BisectingKMeans(n_clusters=n_clusters, random_state=42, bisecting_strategy='largest_cluster')\n",
    "        # df['cluster_id'] = model.fit_predict(coords_scaled)\n",
    "    else:\n",
    "        df['cluster_id'] = 0\n",
    "    \n",
    "    #save each cluster to its own file with original coordinates\n",
    "    saved_files = []\n",
    "    for cid in sorted(df['cluster_id'].unique()):\n",
    "        cluster_df = df[df['cluster_id'] == cid].copy()\n",
    "        \n",
    "        #remove the temporary cluster_id before saving if desired\n",
    "        cluster_df = cluster_df.drop(columns=['cluster_id'])\n",
    "        \n",
    "        file_name = PREPROCESSED_BATCH_DIR.replace(\"idx\", str(cid+1))\n",
    "        if file_name.endswith('.csv'):\n",
    "            cluster_df.to_csv(file_name, index=False)\n",
    "        else:\n",
    "            cluster_df.to_excel(file_name, index=False)\n",
    "            \n",
    "        saved_files.append(file_name)\n",
    "\n",
    "        print(f\"saved {file_name} with {len(cluster_df)} missions.\")\n",
    "    \n",
    "    return saved_files\n",
    "\n",
    "#create_spatial_batches_and_save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874a4b00",
   "metadata": {},
   "source": [
    "### Loss Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "303358fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_loss(predictions, ground_truth, u_batch):\n",
    "    \"\"\"\n",
    "    computes weighted BCE loss for activation, assignment, and sequence heads.\n",
    "    total Loss = Beta * act_loss + alpha * (assign_loss + seq_loss)\n",
    "    \"\"\"\n",
    "    pred_act = predictions['activation']\n",
    "    pred_assign = predictions['assignment']\n",
    "    pred_seq = predictions['sequence']\n",
    "    \n",
    "    #ground truth (should be in [N, 1] shape)\n",
    "    true_act = ground_truth['operator'].y.view(-1, 1)\n",
    "    true_assign = ground_truth['operator', 'assign', 'order'].y.view(-1, 1)\n",
    "    true_seq = ground_truth['order', 'to', 'order'].y.view(-1, 1)\n",
    "    \n",
    "    #BCE losses\n",
    "    loss_act = F.binary_cross_entropy(pred_act, true_act)\n",
    "    loss_assign = F.binary_cross_entropy(pred_assign, true_assign)\n",
    "    loss_seq = F.binary_cross_entropy(pred_seq, true_seq)\n",
    "    \n",
    "    #extract alpha/beta (Mean over batch)\n",
    "    alpha = u_batch[:, 0].mean()\n",
    "    beta = u_batch[:, 1].mean()\n",
    "    \n",
    "    #weighted Sum\n",
    "    #Note that alpha/beta need to be scaled down if they are large (e.g. 100) to prevent explosion\n",
    "    #or rely on the optimizer (Adam) to handle scaling.\n",
    "    total_loss = (beta * loss_act) + (alpha * (loss_assign + loss_seq))\n",
    "    \n",
    "    return total_loss, loss_act.item(), loss_assign.item(), loss_seq.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8afbf91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "found 92 valid schedule instances.\n",
      "Global params extracted from[schedule10M_10_A1.0_B100.0_H60.csv]: Alpha=1.0, Beta=100.0, H_fixed=60.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1/10:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global params extracted from[schedule10M_27_A1.0_B100.0_H90.csv]: Alpha=1.0, Beta=100.0, H_fixed=90.0\n",
      "Global params extracted from[schedule10M_8_A1.0_B100.0_H90.csv]: Alpha=1.0, Beta=100.0, H_fixed=90.0\n",
      "Global params extracted from[schedule10M_56_A1.0_B100.0_H480.csv]: Alpha=1.0, Beta=100.0, H_fixed=480.0\n",
      "Global params extracted from[schedule10M_49_A1.0_B100.0_H480.csv]: Alpha=1.0, Beta=100.0, H_fixed=480.0\n",
      "Global params extracted from[schedule10M_34_A1.0_B100.0_H90.csv]: Alpha=1.0, Beta=100.0, H_fixed=90.0\n",
      "Global params extracted from[schedule10M_6_A1.0_B100.0_H90.csv]: Alpha=1.0, Beta=100.0, H_fixed=90.0\n",
      "Global params extracted from[schedule10M_86_A1.0_B100.0_H480.csv]: Alpha=1.0, Beta=100.0, H_fixed=480.0\n",
      "Global params extracted from[schedule10M_81_A1.0_B100.0_H90.csv]: Alpha=1.0, Beta=100.0, H_fixed=90.0\n",
      "Global params extracted from[schedule10M_41_A1.0_B100.0_H120.csv]: Alpha=1.0, Beta=100.0, H_fixed=120.0\n",
      "Global params extracted from[schedule10M_16_A1.0_B100.0_H480.csv]: Alpha=1.0, Beta=100.0, H_fixed=480.0\n",
      "Global params extracted from[schedule10M_12_A1.0_B100.0_H90.csv]: Alpha=1.0, Beta=100.0, H_fixed=90.0\n",
      "Global params extracted from[schedule10M_8_A1.0_B100.0_H120.csv]: Alpha=1.0, Beta=100.0, H_fixed=120.0\n",
      "Global params extracted from[schedule10M_20_A1.0_B100.0_H60.csv]: Alpha=1.0, Beta=100.0, H_fixed=60.0\n",
      "Global params extracted from[schedule10M_1_A1.0_B100.0_H90.csv]: Alpha=1.0, Beta=100.0, H_fixed=90.0\n",
      "Global params extracted from[schedule10M_43_A1.0_B100.0_H480.csv]: Alpha=1.0, Beta=100.0, H_fixed=480.0\n",
      "Global params extracted from[schedule10M_22_A1.0_B100.0_H90.csv]: Alpha=1.0, Beta=100.0, H_fixed=90.0\n",
      "Global params extracted from[schedule10M_3_A1.0_B100.0_H60.csv]: Alpha=1.0, Beta=100.0, H_fixed=60.0\n",
      "Global params extracted from[schedule10M_50_A1.0_B100.0_H120.csv]: Alpha=1.0, Beta=100.0, H_fixed=120.0\n",
      "Global params extracted from[schedule10M_35_A1.0_B100.0_H480.csv]: Alpha=1.0, Beta=100.0, H_fixed=480.0\n",
      "Global params extracted from[schedule10M_70_A1.0_B100.0_H120.csv]: Alpha=1.0, Beta=100.0, H_fixed=120.0\n",
      "Global params extracted from[schedule10M_57_A1.0_B100.0_H90.csv]: Alpha=1.0, Beta=100.0, H_fixed=90.0\n",
      "Global params extracted from[schedule10M_46_A1.0_B100.0_H120.csv]: Alpha=1.0, Beta=100.0, H_fixed=120.0\n",
      "Global params extracted from[schedule10M_48_A1.0_B100.0_H480.csv]: Alpha=1.0, Beta=100.0, H_fixed=480.0\n",
      "Global params extracted from[schedule10M_61_A1.0_B100.0_H60.csv]: Alpha=1.0, Beta=100.0, H_fixed=60.0\n",
      "Global params extracted from[schedule10M_92_A1.0_B100.0_H90.csv]: Alpha=1.0, Beta=100.0, H_fixed=90.0\n",
      "Global params extracted from[schedule10M_19_A1.0_B100.0_H60.csv]: Alpha=1.0, Beta=100.0, H_fixed=60.0\n",
      "Global params extracted from[schedule10M_91_A1.0_B100.0_H480.csv]: Alpha=1.0, Beta=100.0, H_fixed=480.0\n",
      "Global params extracted from[schedule10M_5_A1.0_B100.0_H60.csv]: Alpha=1.0, Beta=100.0, H_fixed=60.0\n",
      "Global params extracted from[schedule10M_10_A1.0_B100.0_H60.csv]: Alpha=1.0, Beta=100.0, H_fixed=60.0\n",
      "Global params extracted from[schedule10M_82_A1.0_B100.0_H90.csv]: Alpha=1.0, Beta=100.0, H_fixed=90.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1/10:   0%|          | 0/1 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m total_epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mdataset\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(loader), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(loader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     37\u001b[0m     batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     38\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\abudo\\source\\vscode_projects\\ManualWarehouseOptimization\\cplex_env\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abudo\\source\\vscode_projects\\ManualWarehouseOptimization\\cplex_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    738\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\abudo\\source\\vscode_projects\\ManualWarehouseOptimization\\cplex_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    787\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 788\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    789\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    790\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\abudo\\source\\vscode_projects\\ManualWarehouseOptimization\\cplex_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\abudo\\source\\vscode_projects\\ManualWarehouseOptimization\\cplex_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\abudo\\source\\vscode_projects\\ManualWarehouseOptimization\\GnnScheduleDataset.py:74\u001b[0m, in \u001b[0;36mGnnScheduleDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     71\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems[idx]\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m#load graph HeteroData instance\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_and_process_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpallet_types_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpallet_types_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfork_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43medge\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschedule_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mschedule\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\abudo\\source\\vscode_projects\\ManualWarehouseOptimization\\GnnDataInstanceBuilder.py:88\u001b[0m, in \u001b[0;36mGnnDataInstanceBuilder.load_and_process_data\u001b[1;34m(self, node_file_path, pallet_types_file_path, operator_file_path, edge_file_path, schedule_file_path)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m missions_feature \u001b[38;5;129;01min\u001b[39;00m missions_features:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df_missions[missions_feature]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m df_missions[missions_feature]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m'\u001b[39m:  \u001b[38;5;66;03m#only apply to string/object columns\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m         df_missions[missions_feature] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_missions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmissions_feature\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m mission_feats_raw \u001b[38;5;241m=\u001b[39m df_missions[missions_features]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     91\u001b[0m missions_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(mission_feats_raw)\n",
      "File \u001b[1;32mc:\\Users\\abudo\\source\\vscode_projects\\ManualWarehouseOptimization\\cplex_env\\lib\\site-packages\\pandas\\core\\generic.py:6665\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   6659\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   6660\u001b[0m         ser\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   6661\u001b[0m     ]\n\u001b[0;32m   6663\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6664\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[1;32m-> 6665\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6666\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m   6667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\abudo\\source\\vscode_projects\\ManualWarehouseOptimization\\cplex_env\\lib\\site-packages\\pandas\\core\\internals\\managers.py:449\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    447\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mastype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abudo\\source\\vscode_projects\\ManualWarehouseOptimization\\cplex_env\\lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32mc:\\Users\\abudo\\source\\vscode_projects\\ManualWarehouseOptimization\\cplex_env\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:784\u001b[0m, in \u001b[0;36mBlock.astype\u001b[1;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[0;32m    781\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not squeeze with more than one column.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    782\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m--> 784\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    786\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[0;32m    788\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abudo\\source\\vscode_projects\\ManualWarehouseOptimization\\cplex_env\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:237\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[1;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    234\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 237\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\abudo\\source\\vscode_projects\\ManualWarehouseOptimization\\cplex_env\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:182\u001b[0m, in \u001b[0;36mastype_array\u001b[1;34m(values, dtype, copy)\u001b[0m\n\u001b[0;32m    179\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 182\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\abudo\\source\\vscode_projects\\ManualWarehouseOptimization\\cplex_env\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:133\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "#init dataset\n",
    "dataset = GnnScheduleDataset(\n",
    "    schedule_dir=SCHEDULE_DIR,\n",
    "    mission_base_path=MISSION_BATCH_DIR,\n",
    "    edge_base_path=MISSION_BATCH_TRAVEL_DIR,\n",
    "    pallet_types_file_path=UDC_TYPES_DIR,\n",
    "    fork_path=FORK_LIFTS_DIR\n",
    ")\n",
    "\n",
    "print(f\"found {len(dataset)} valid schedule instances.\")\n",
    "\n",
    "#create DataLoader using the dataset\n",
    "#batch_size can be > 1 to train on multiple graphs at once\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "#init model\n",
    "if len(dataset) > 0:\n",
    "    sample_data = dataset[0]\n",
    "    model = MultiCriteriaGNNModel(\n",
    "        metadata=sample_data.metadata(),\n",
    "        hidden_dim=64,\n",
    "        num_layers=3,\n",
    "        heads=4\n",
    "    ).to(device)\n",
    "\n",
    "    #adam optimizer is a standard for GNNs\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        model.train()\n",
    "        total_epoch_loss = 0.0\n",
    "        x = loader.dataset\n",
    "        for batch_idx, batch in tqdm(enumerate(loader), total=len(loader), desc=f\"epoch {epoch}/{NUM_EPOCHS}\"):\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #construct batch_dict\n",
    "            batch_dict_arg = {\n",
    "                'operator': batch['operator'].batch,\n",
    "                'order': batch['order'].batch\n",
    "            }\n",
    "            \n",
    "            #forward pass\n",
    "            preds = model(\n",
    "                batch.x_dict, \n",
    "                batch.edge_index_dict, \n",
    "                batch.edge_attr_dict,\n",
    "                batch.u,\n",
    "                batch_dict=batch_dict_arg\n",
    "            )\n",
    "            \n",
    "            #backward step and optimization\n",
    "            loss, l_act, l_assign, l_seq = weighted_loss(preds, batch, batch.u)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_epoch_loss += loss.item()\n",
    "            \n",
    "            #print mini-batch progress (every 2 batches)\n",
    "            if batch_idx % 2 == 0:\n",
    "                print(f\"[Batch {batch_idx}] loss: {loss.item():.4f} (act_loss: {l_act:.3f}, assign_loss: {l_assign:.3f}, seq_loss: {l_seq:.3f})\")\n",
    "        \n",
    "        avg_loss = total_epoch_loss / len(loader)\n",
    "        print(f\"Epoch {epoch} complete. average loss: {avg_loss:.4f}\")\n",
    "            \n",
    "        # print(f\"Batch {batch_idx}:\")\n",
    "        # print(f\"Batch Size: {batch.num_graphs}\")\n",
    "        # print(f\"Activation Probs: {out['activation']}\")\n",
    "        # print(f\"Assignment Probs: {out['assignment']}\")\n",
    "        # print(f\"Sequence Probs: {out['sequence']}\")\n",
    "        \n",
    "        #if batch_idx >= 1: break #limit to 2 batches, just for demo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cplex_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
