{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e43a612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import importlib\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "import GnnScheduleDataset as GnnScheduleDataset_Module\n",
    "import MultiCriteriaGNNModel as MultiCriteriaGNNModel_Module\n",
    "\n",
    "importlib.reload(GnnScheduleDataset_Module) # in case of updates\n",
    "importlib.reload(MultiCriteriaGNNModel_Module) # in case of updates\n",
    "\n",
    "from GnnScheduleDataset import GnnScheduleDataset\n",
    "from MultiCriteriaGNNModel import MultiCriteriaGNNModel\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "#manual installing\n",
    "def install_package(package_name, use_index_url=True):\n",
    "    print(f\"Installing {package_name}...\")\n",
    "    #run: python.exe -m pip install [package_name]\n",
    "\n",
    "    subprocess.run(\n",
    "        [sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", package_name],\n",
    "        check=True,\n",
    "        text=True\n",
    "    )\n",
    "    if use_index_url:\n",
    "        subprocess.run(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", package_name, \"--index-url\", \"https://download.pytorch.org/whl/cu126\"],\n",
    "            check=True,\n",
    "            text=True\n",
    "        )\n",
    "    else:\n",
    "        subprocess.run(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", package_name],\n",
    "            check=True,\n",
    "            text=True\n",
    "        )\n",
    "    print(f\"Successfully installed {package_name}!\")\n",
    "\n",
    "# Try to import, if it fails, install it\n",
    "# try:\n",
    "#     import torch\n",
    "#     print(\"Torch is already available.\")\n",
    "# except: \n",
    "#     #install_package('torch')\n",
    "#     # After installing, you must use importlib to refresh or restart the script\n",
    "#     import torch\n",
    "#     print(\"Torch imported successfully after installation.\")\n",
    "\n",
    "#torch-scatter torch-sparse torch-cluster torch-spline-conv pyg-lib\n",
    "#torch-scatter torch-sparse torch-cluster torch-spline-conv\n",
    "# install_package('torch-scatter')\n",
    "# install_package('torch-sparse')\n",
    "# install_package('torch-cluster')\n",
    "# install_package('torch-spline-conv')\n",
    "\n",
    "# try:\n",
    "#     import torch_geometric\n",
    "#     print(\"Torch is already available.\")\n",
    "# except: \n",
    "#     install_package('torch_geometric', False)\n",
    "#     # After installing, you must use importlib to refresh or restart the script\n",
    "#     import torch_geometric\n",
    "#     print(\"Torch imported successfully after installation.\")\n",
    "\n",
    "\n",
    "#file paths\n",
    "MISSION_BATCH_DIR = \"./datasets/mini-batch/Batch10M_distanced.csv\"\n",
    "UDC_TYPES_DIR = \"./datasets/WM_UDC_TYPE.csv\"\n",
    "MISSION_BATCH_TRAVEL_DIR = \"./datasets/mini-batch/Batch10M_travel_distanced.csv\"\n",
    "FORK_LIFTS_DIR = \"./datasets/ForkLifts10W.csv\"\n",
    "#MISSION_TYPES_DIR = \"./datasets/MissionTypes.csv\"\n",
    "SCHEDULE_DIR = \"./schedules/mini-batch/\"\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 8 #nice to be equal to the number of mini-batch instances\n",
    "LEARNING_RATE = 0.001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874a4b00",
   "metadata": {},
   "source": [
    "### Loss Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "303358fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_loss(predictions, ground_truth, u_batch):\n",
    "    \"\"\"\n",
    "    computes weighted BCE loss for activation, assignment, and sequence heads.\n",
    "    total Loss = Beta * act_loss + alpha * (assign_loss + seq_loss)\n",
    "    \"\"\"\n",
    "    pred_act = predictions['activation']\n",
    "    pred_assign = predictions['assignment']\n",
    "    pred_seq = predictions['sequence']\n",
    "    \n",
    "    #ground truth (should be in [N, 1] shape)\n",
    "    true_act = ground_truth['operator'].y.view(-1, 1)\n",
    "    true_assign = ground_truth['operator', 'assign', 'order'].y.view(-1, 1)\n",
    "    true_seq = ground_truth['order', 'to', 'order'].y.view(-1, 1)\n",
    "    \n",
    "    #BCE losses\n",
    "    loss_act = F.binary_cross_entropy(pred_act, true_act)\n",
    "    loss_assign = F.binary_cross_entropy(pred_assign, true_assign)\n",
    "    loss_seq = F.binary_cross_entropy(pred_seq, true_seq)\n",
    "    \n",
    "    #extract alpha/beta (Mean over batch)\n",
    "    alpha = u_batch[:, 0].mean()\n",
    "    beta = u_batch[:, 1].mean()\n",
    "    \n",
    "    #weighted Sum\n",
    "    #Note that alpha/beta need to be scaled down if they are large (e.g. 100) to prevent explosion\n",
    "    #or rely on the optimizer (Adam) to handle scaling.\n",
    "    total_loss = (beta * loss_act) + (alpha * (loss_assign + loss_seq))\n",
    "    \n",
    "    return total_loss, loss_act.item(), loss_assign.item(), loss_seq.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8afbf91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "found 9 valid schedule instances.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1/10: 100%|██████████| 2/2 [00:00<00:00,  7.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 0] loss: 75.3580 (act_loss: 0.727, assign_loss: 2.331, seq_loss: 0.316)\n",
      "Epoch 1 complete. average loss: 69.9799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2/10: 100%|██████████| 2/2 [00:00<00:00,  8.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 0] loss: 74.0735 (act_loss: 0.724, assign_loss: 1.389, seq_loss: 0.253)\n",
      "Epoch 2 complete. average loss: 82.4990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3/10: 100%|██████████| 2/2 [00:00<00:00,  8.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 0] loss: 70.0956 (act_loss: 0.691, assign_loss: 0.749, seq_loss: 0.266)\n",
      "Epoch 3 complete. average loss: 73.4632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4/10: 100%|██████████| 2/2 [00:00<00:00,  8.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 0] loss: 68.3060 (act_loss: 0.676, assign_loss: 0.442, seq_loss: 0.262)\n",
      "Epoch 4 complete. average loss: 67.8510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5/10: 100%|██████████| 2/2 [00:00<00:00,  8.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 0] loss: 68.6943 (act_loss: 0.682, assign_loss: 0.241, seq_loss: 0.271)\n",
      "Epoch 5 complete. average loss: 69.2531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6/10: 100%|██████████| 2/2 [00:00<00:00,  7.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 0] loss: 66.5053 (act_loss: 0.661, assign_loss: 0.181, seq_loss: 0.258)\n",
      "Epoch 6 complete. average loss: 68.2081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 7/10: 100%|██████████| 2/2 [00:00<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 0] loss: 65.5304 (act_loss: 0.651, assign_loss: 0.171, seq_loss: 0.245)\n",
      "Epoch 7 complete. average loss: 63.9690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 8/10: 100%|██████████| 2/2 [00:00<00:00,  8.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 0] loss: 66.4515 (act_loss: 0.660, assign_loss: 0.172, seq_loss: 0.246)\n",
      "Epoch 8 complete. average loss: 58.7624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 9/10: 100%|██████████| 2/2 [00:00<00:00,  7.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 0] loss: 66.0396 (act_loss: 0.656, assign_loss: 0.174, seq_loss: 0.253)\n",
      "Epoch 9 complete. average loss: 63.8811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 10/10: 100%|██████████| 2/2 [00:00<00:00,  8.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 0] loss: 68.1502 (act_loss: 0.677, assign_loss: 0.183, seq_loss: 0.244)\n",
      "Epoch 10 complete. average loss: 59.3330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "#init dataset\n",
    "dataset = GnnScheduleDataset(\n",
    "    schedule_dir=SCHEDULE_DIR,\n",
    "    mission_base_path=MISSION_BATCH_DIR,\n",
    "    edge_base_path=MISSION_BATCH_TRAVEL_DIR,\n",
    "    pallet_types_file_path=UDC_TYPES_DIR,\n",
    "    fork_path=FORK_LIFTS_DIR\n",
    ")\n",
    "\n",
    "print(f\"found {len(dataset)} valid schedule instances.\")\n",
    "\n",
    "#create DataLoader using the dataset\n",
    "#batch_size can be > 1 to train on multiple graphs at once\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "#init model\n",
    "if len(dataset) > 0:\n",
    "    sample_data = dataset[0]\n",
    "    model = MultiCriteriaGNNModel(\n",
    "        metadata=sample_data.metadata(),\n",
    "        hidden_dim=64,\n",
    "        num_layers=3,\n",
    "        heads=4\n",
    "    ).to(device)\n",
    "\n",
    "    #adam optimizer is a standard for GNNs\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        model.train()\n",
    "        total_epoch_loss = 0.0\n",
    "        \n",
    "        for batch_idx, batch in tqdm(enumerate(loader), total=len(loader), desc=f\"epoch {epoch}/{NUM_EPOCHS}\"):\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #construct batch_dict\n",
    "            batch_dict_arg = {\n",
    "                'operator': batch['operator'].batch,\n",
    "                'order': batch['order'].batch\n",
    "            }\n",
    "            \n",
    "            #forward pass\n",
    "            preds = model(\n",
    "                batch.x_dict, \n",
    "                batch.edge_index_dict, \n",
    "                batch.edge_attr_dict,\n",
    "                batch.u,\n",
    "                batch_dict=batch_dict_arg\n",
    "            )\n",
    "            \n",
    "            #backward step and optimization\n",
    "            loss, l_act, l_assign, l_seq = weighted_loss(preds, batch, batch.u)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_epoch_loss += loss.item()\n",
    "            \n",
    "            #print mini-batch progress (every 2 batches)\n",
    "            if batch_idx % 2 == 0:\n",
    "                print(f\"[Batch {batch_idx}] loss: {loss.item():.4f} (act_loss: {l_act:.3f}, assign_loss: {l_assign:.3f}, seq_loss: {l_seq:.3f})\")\n",
    "        \n",
    "        avg_loss = total_epoch_loss / len(loader)\n",
    "        print(f\"Epoch {epoch} complete. average loss: {avg_loss:.4f}\")\n",
    "            \n",
    "        # print(f\"Batch {batch_idx}:\")\n",
    "        # print(f\"Batch Size: {batch.num_graphs}\")\n",
    "        # print(f\"Activation Probs: {out['activation']}\")\n",
    "        # print(f\"Assignment Probs: {out['assignment']}\")\n",
    "        # print(f\"Sequence Probs: {out['sequence']}\")\n",
    "        \n",
    "        #if batch_idx >= 1: break #limit to 2 batches, just for demo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cplex_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
