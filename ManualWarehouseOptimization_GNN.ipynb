{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e43a612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abudo\\source\\vscode_projects\\ManualWarehouseOptimization\\cplex_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, BisectingKMeans\n",
    "import importlib\n",
    "import math\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from k_means_constrained import KMeansConstrained\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "import GnnScheduleDataset as GnnScheduleDataset_Module\n",
    "import MultiCriteriaGNNModel as MultiCriteriaGNNModel_Module\n",
    "\n",
    "importlib.reload(GnnScheduleDataset_Module) # in case of updates\n",
    "importlib.reload(MultiCriteriaGNNModel_Module) # in case of updates\n",
    "\n",
    "from GnnScheduleDataset import GnnScheduleDataset\n",
    "from MultiCriteriaGNNModel import MultiCriteriaGNNModel\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "#manual installing\n",
    "def install_package(package_name, use_index_url=True):\n",
    "    print(f\"Installing {package_name}...\")\n",
    "    #run: python.exe -m pip install [package_name]\n",
    "\n",
    "    subprocess.run(\n",
    "        [sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", package_name],\n",
    "        check=True,\n",
    "        text=True\n",
    "    )\n",
    "    if use_index_url:\n",
    "        subprocess.run(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", package_name, \"--index-url\", \"https://download.pytorch.org/whl/cu126\"],\n",
    "            check=True,\n",
    "            text=True\n",
    "        )\n",
    "    else:\n",
    "        subprocess.run(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", package_name],\n",
    "            check=True,\n",
    "            text=True\n",
    "        )\n",
    "    print(f\"Successfully installed {package_name}!\")\n",
    "\n",
    "# Try to import, if it fails, install it\n",
    "# try:\n",
    "#     import torch\n",
    "#     print(\"Torch is already available.\")\n",
    "# except: \n",
    "#     #install_package('torch')\n",
    "#     # After installing, you must use importlib to refresh or restart the script\n",
    "#     import torch\n",
    "#     print(\"Torch imported successfully after installation.\")\n",
    "\n",
    "#torch-scatter torch-sparse torch-cluster torch-spline-conv pyg-lib\n",
    "#torch-scatter torch-sparse torch-cluster torch-spline-conv\n",
    "# install_package('torch-scatter')\n",
    "# install_package('torch-sparse')\n",
    "# install_package('torch-cluster')\n",
    "# install_package('torch-spline-conv')\n",
    "\n",
    "# try:\n",
    "#     import torch_geometric\n",
    "#     print(\"Torch is already available.\")\n",
    "# except: \n",
    "#     install_package('torch_geometric', False)\n",
    "#     # After installing, you must use importlib to refresh or restart the script\n",
    "#     import torch_geometric\n",
    "#     print(\"Torch imported successfully after installation.\")\n",
    "\n",
    "#Try to import, if it fails, install it\n",
    "# try:\n",
    "#     import k_means_constrained\n",
    "#     print(\"k_means_constrained is already available.\")\n",
    "# except: \n",
    "#     install_package('k_means_constrained', False)\n",
    "#     # After installing, you must use importlib to refresh or restart the script\n",
    "#     import torch\n",
    "#     print(\"k_means_constrained imported successfully after installation.\")\n",
    "\n",
    "#file paths\n",
    "TARGET_BATCH_SIZE = 10 #number of missions per mini-batch\n",
    "LARGE_SCALE_MISSION_BATCH_DIR = \"./datasets/Batch1000M.csv\"\n",
    "PREPROCESSED_BATCH_DIR = f\"./preprocessed/batch1000M/Batch{TARGET_BATCH_SIZE}M_idx.xlsx\" #idx to be replaced cluster idx\n",
    "MISSION_BATCH_DIR = \"./datasets/mini-batch/Batch10M_distanced.csv\"\n",
    "UDC_TYPES_DIR = \"./datasets/WM_UDC_TYPE.csv\"\n",
    "MISSION_BATCH_TRAVEL_DIR = \"./datasets/mini-batch/Batch10M_travel_distanced.csv\"\n",
    "FORK_LIFTS_DIR = \"./datasets/ForkLifts10W.csv\"\n",
    "#MISSION_TYPES_DIR = \"./datasets/MissionTypes.csv\"\n",
    "SCHEDULE_DIR = \"./schedules/mini-batch/\"\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 92 #nice to be equal to the number of mini-batch instances\n",
    "LEARNING_RATE = 0.001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0325e6",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Split the large-scale batch in mini-batches (ex. 10 missions each) using spatial k-means, considering only `FROM_X, FROM_Y, FROM_Z, TO_X, TO_Y, TO_Z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05824876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total orders: 920 | target clusters: 92\n",
      "saved ./preprocessed/batch1000M/Batch10M_1.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_2.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_3.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_4.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_5.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_6.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_7.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_8.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_9.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_10.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_11.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_12.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_13.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_14.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_15.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_16.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_17.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_18.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_19.xlsx with 10 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_20.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_21.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_22.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_23.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_24.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_25.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_26.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_27.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_28.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_29.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_30.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_31.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_32.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_33.xlsx with 10 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_34.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_35.xlsx with 10 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_36.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_37.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_38.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_39.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_40.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_41.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_42.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_43.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_44.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_45.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_46.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_47.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_48.xlsx with 10 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_49.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_50.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_51.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_52.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_53.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_54.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_55.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_56.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_57.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_58.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_59.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_60.xlsx with 10 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_61.xlsx with 10 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_62.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_63.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_64.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_65.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_66.xlsx with 10 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_67.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_68.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_69.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_70.xlsx with 10 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_71.xlsx with 10 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_72.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_73.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_74.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_75.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_76.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_77.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_78.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_79.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_80.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_81.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_82.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_83.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_84.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_85.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_86.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_87.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_88.xlsx with 10 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_89.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_90.xlsx with 11 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_91.xlsx with 9 missions.\n",
      "saved ./preprocessed/batch1000M/Batch10M_92.xlsx with 9 missions.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./preprocessed/batch1000M/Batch10M_1.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_2.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_3.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_4.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_5.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_6.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_7.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_8.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_9.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_10.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_11.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_12.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_13.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_14.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_15.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_16.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_17.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_18.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_19.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_20.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_21.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_22.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_23.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_24.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_25.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_26.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_27.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_28.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_29.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_30.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_31.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_32.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_33.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_34.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_35.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_36.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_37.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_38.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_39.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_40.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_41.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_42.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_43.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_44.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_45.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_46.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_47.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_48.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_49.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_50.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_51.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_52.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_53.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_54.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_55.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_56.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_57.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_58.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_59.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_60.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_61.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_62.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_63.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_64.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_65.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_66.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_67.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_68.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_69.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_70.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_71.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_72.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_73.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_74.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_75.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_76.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_77.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_78.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_79.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_80.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_81.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_82.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_83.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_84.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_85.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_86.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_87.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_88.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_89.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_90.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_91.xlsx',\n",
       " './preprocessed/batch1000M/Batch10M_92.xlsx']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_spatial_batches_and_save(input_csv=LARGE_SCALE_MISSION_BATCH_DIR, target_batch_size=TARGET_BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    groups missions into spatially dense clusters of <50 missions \n",
    "    and saves each group to a separate CSV file.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(PREPROCESSED_BATCH_DIR), exist_ok=True)\n",
    "    \n",
    "    #load the main dataset\n",
    "    df = pd.read_csv(input_csv).drop_duplicates(subset=['CD_MISSION'], keep='first')\n",
    "    df = df.reset_index(drop=True)\n",
    "    n_samples = len(df)\n",
    "    \n",
    "    #determine number of clusters\n",
    "    n_clusters = max(1, math.ceil(n_samples / target_batch_size))\n",
    "    print(f\"total orders: {n_samples} | target clusters: {n_clusters}\")\n",
    "\n",
    "    if n_clusters == 1:\n",
    "        df['cluster_id'] = 0\n",
    "        df.to_csv('cluster_0.csv', index=False)\n",
    "        return [\"cluster_0.csv\"]\n",
    "\n",
    "    #feature Selection: 6D spatial coordinates\n",
    "    #use both pickup (from) and drop-off (to) locations\n",
    "    coord_cols = ['FROM_X', 'FROM_Y', 'FROM_Z', 'TO_X', 'TO_Y', 'TO_Z']\n",
    "    \n",
    "    for col in coord_cols:\n",
    "        if df[col].dtype == 'object' or df[col].dtype == 'string':  #only apply to string/object columns\n",
    "            df[col] = df[col].str.replace(',', '', regex=False)\n",
    "\n",
    "    coords = df[coord_cols].astype(float).fillna(0).values\n",
    "    \n",
    "    #standardize (crucial for K-Means distance calculations)\n",
    "    scaler = StandardScaler()\n",
    "    coords_scaled = scaler.fit_transform(coords)\n",
    "    \n",
    "    #cluster logic\n",
    "    if n_clusters > 1:\n",
    "        #kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10) #produces some very unbalanced clusters\n",
    "        #df['cluster_id'] = kmeans.fit_predict(coords_scaled)\n",
    "\n",
    "        clf = KMeansConstrained(\n",
    "            n_clusters=n_clusters,\n",
    "            size_min=target_batch_size - 1,\n",
    "            size_max=target_batch_size + 1,\n",
    "            random_state=42\n",
    "        )\n",
    "        df['cluster_id'] = clf.fit_predict(coords_scaled)\n",
    "\n",
    "        # model = BisectingKMeans(n_clusters=n_clusters, random_state=42, bisecting_strategy='largest_cluster')\n",
    "        # df['cluster_id'] = model.fit_predict(coords_scaled)\n",
    "    else:\n",
    "        df['cluster_id'] = 0\n",
    "    \n",
    "    #save each cluster to its own file with original coordinates\n",
    "    saved_files = []\n",
    "    for cid in sorted(df['cluster_id'].unique()):\n",
    "        cluster_df = df[df['cluster_id'] == cid].copy()\n",
    "        \n",
    "        #remove the temporary cluster_id before saving if desired\n",
    "        cluster_df = cluster_df.drop(columns=['cluster_id'])\n",
    "        \n",
    "        file_name = PREPROCESSED_BATCH_DIR.replace(\"idx\", str(cid+1))\n",
    "        if file_name.endswith('.csv'):\n",
    "            cluster_df.to_csv(file_name, index=False)\n",
    "        else:\n",
    "            cluster_df.to_excel(file_name, index=False)\n",
    "            \n",
    "        saved_files.append(file_name)\n",
    "\n",
    "        print(f\"saved {file_name} with {len(cluster_df)} missions.\")\n",
    "    \n",
    "    return saved_files\n",
    "\n",
    "#create_spatial_batches_and_save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874a4b00",
   "metadata": {},
   "source": [
    "### Loss Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "303358fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_loss(predictions, ground_truth, u_batch):\n",
    "    \"\"\"\n",
    "    computes weighted BCE loss for activation, assignment, and sequence heads.\n",
    "    total Loss = Beta * act_loss + alpha * (assign_loss + seq_loss)\n",
    "    \"\"\"\n",
    "    pred_act = predictions['activation']\n",
    "    pred_assign = predictions['assignment']\n",
    "    pred_seq = predictions['sequence']\n",
    "    \n",
    "    #ground truth (should be in [N, 1] shape)\n",
    "    true_act = ground_truth['operator'].y.view(-1, 1)\n",
    "    true_assign = ground_truth['operator', 'assign', 'order'].y.view(-1, 1)\n",
    "    true_seq = ground_truth['order', 'to', 'order'].y.view(-1, 1)\n",
    "    \n",
    "    #BCE losses\n",
    "    loss_act = F.binary_cross_entropy(pred_act, true_act)\n",
    "    loss_assign = F.binary_cross_entropy(pred_assign, true_assign)\n",
    "    loss_seq = F.binary_cross_entropy(pred_seq, true_seq)\n",
    "    \n",
    "    #extract alpha/beta (Mean over batch)\n",
    "    alpha = u_batch[:, 0].mean()\n",
    "    beta = u_batch[:, 1].mean()\n",
    "    \n",
    "    #weighted Sum\n",
    "    #Note that alpha/beta need to be scaled down if they are large (e.g. 100) to prevent explosion\n",
    "    #or rely on the optimizer (Adam) to handle scaling.\n",
    "    total_loss = (beta * loss_act) + (alpha * (loss_assign + loss_seq))\n",
    "    \n",
    "    return total_loss, loss_act.item(), loss_assign.item(), loss_seq.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8afbf91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "found 194 valid schedule instances.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1/10: 100%|██████████| 3/3 [00:04<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] loss: 513.9634 (act_loss: 5.134, assign_loss: 0.139, seq_loss: 0.410)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2/10: 100%|██████████| 3/3 [00:03<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] loss: 241.1121 (act_loss: 2.406, assign_loss: 0.117, seq_loss: 0.423)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3/10: 100%|██████████| 3/3 [00:03<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] loss: 91.4570 (act_loss: 0.910, assign_loss: 0.111, seq_loss: 0.334)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4/10: 100%|██████████| 3/3 [00:03<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] loss: 62.3383 (act_loss: 0.618, assign_loss: 0.116, seq_loss: 0.402)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5/10: 100%|██████████| 3/3 [00:03<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] loss: 76.2262 (act_loss: 0.758, assign_loss: 0.142, seq_loss: 0.318)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6/10: 100%|██████████| 3/3 [00:03<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] loss: 91.8865 (act_loss: 0.914, assign_loss: 0.114, seq_loss: 0.351)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 7/10: 100%|██████████| 3/3 [00:03<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] loss: 105.0797 (act_loss: 1.047, assign_loss: 0.112, seq_loss: 0.315)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 8/10: 100%|██████████| 3/3 [00:03<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] loss: 97.9746 (act_loss: 0.975, assign_loss: 0.102, seq_loss: 0.344)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 9/10: 100%|██████████| 3/3 [00:04<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] loss: 58.5897 (act_loss: 0.581, assign_loss: 0.118, seq_loss: 0.346)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 10/10: 100%|██████████| 3/3 [00:03<00:00,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] loss: 82.5789 (act_loss: 0.821, assign_loss: 0.123, seq_loss: 0.354)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "#init dataset\n",
    "dataset = GnnScheduleDataset(\n",
    "    schedule_dir=SCHEDULE_DIR,\n",
    "    mission_base_path=MISSION_BATCH_DIR,\n",
    "    edge_base_path=MISSION_BATCH_TRAVEL_DIR,\n",
    "    pallet_types_file_path=UDC_TYPES_DIR,\n",
    "    fork_path=FORK_LIFTS_DIR\n",
    ")\n",
    "\n",
    "print(f\"found {len(dataset)} valid schedule instances.\")\n",
    "\n",
    "#create DataLoader using the dataset\n",
    "#batch_size can be > 1 to train on multiple graphs at once\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "#init model\n",
    "if len(dataset) > 0:\n",
    "    sample_data = dataset[0]\n",
    "    model = MultiCriteriaGNNModel(\n",
    "        metadata=sample_data.metadata(),\n",
    "        hidden_dim=64,\n",
    "        num_layers=3,\n",
    "        heads=4\n",
    "    ).to(device)\n",
    "\n",
    "    #adam optimizer is a standard for GNNs\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        model.train()\n",
    "        total_epoch_loss = 0.0\n",
    "        x = loader.dataset\n",
    "        for batch_idx, batch in tqdm(enumerate(loader), total=len(loader), desc=f\"epoch {epoch}/{NUM_EPOCHS}\"):\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #construct batch_dict\n",
    "            batch_dict_arg = {\n",
    "                'operator': batch['operator'].batch,\n",
    "                'order': batch['order'].batch\n",
    "            }\n",
    "            \n",
    "            #forward pass\n",
    "            preds = model(\n",
    "                batch.x_dict, \n",
    "                batch.edge_index_dict, \n",
    "                batch.edge_attr_dict,\n",
    "                batch.u,\n",
    "                batch_dict=batch_dict_arg\n",
    "            )\n",
    "            \n",
    "            #backward step and optimization\n",
    "            loss, l_act, l_assign, l_seq = weighted_loss(preds, batch, batch.u)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_epoch_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_epoch_loss / len(loader)\n",
    "        #print(f\"Epoch {epoch} complete. average loss: {avg_loss:.4f}\")\n",
    "        print(f\"[Epoch {epoch}] loss: {loss.item():.4f} (act_loss: {l_act:.3f}, assign_loss: {l_assign:.3f}, seq_loss: {l_seq:.3f})\")\n",
    "            \n",
    "        # print(f\"Batch {batch_idx}:\")\n",
    "        # print(f\"Batch Size: {batch.num_graphs}\")\n",
    "        # print(f\"Activation Probs: {out['activation']}\")\n",
    "        # print(f\"Assignment Probs: {out['assignment']}\")\n",
    "        # print(f\"Sequence Probs: {out['sequence']}\")\n",
    "        \n",
    "        #if batch_idx >= 1: break #limit to 2 batches, just for demo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cplex_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
